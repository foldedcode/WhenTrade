"""
Celeryä»»åŠ¡å®šä¹‰
"""
from datetime import datetime, timedelta
from celery import shared_task
from celery.utils.log import get_task_logger
import psutil
import redis
from sqlalchemy import text
from core.database import get_db
from core.services.redis_pubsub import redis_publisher
from core.graph.whentrade_graph import WhenTradeGraph
from core.services.llm_config_service import llm_config_service
from core.default_config import WHENTRADE_CONFIG
import traceback

logger = get_task_logger(__name__)


@shared_task
def check_system_health():
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        # æ£€æŸ¥CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
        disk = psutil.disk_usage('/')
        disk_percent = disk.percent
        
        health_status = {
            'timestamp': datetime.utcnow().isoformat(),
            'cpu_percent': cpu_percent,
            'memory_percent': memory_percent,
            'disk_percent': disk_percent,
            'status': 'healthy'
        }
        
        # åˆ¤æ–­æ˜¯å¦æœ‰å‘Šè­¦
        if cpu_percent > 80 or memory_percent > 80 or disk_percent > 80:
            health_status['status'] = 'warning'
            logger.warning(f"System health warning: {health_status}")
        else:
            logger.info(f"System health check: {health_status}")
        
        return health_status
        
    except Exception as e:
        logger.error(f"Error checking system health: {str(e)}")
        return {'status': 'error', 'error': str(e)}


@shared_task
def cleanup_old_data():
    """æ¸…ç†æ—§æ•°æ®"""
    try:
        # æ¸…ç†30å¤©å‰çš„æ—¥å¿—æ•°æ®
        cutoff_date = datetime.utcnow() - timedelta(days=30)
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„æ¸…ç†é€»è¾‘
        # ä¾‹å¦‚ï¼šåˆ é™¤æ—§çš„åˆ†æä»»åŠ¡è®°å½•ã€æ—¥å¿—ç­‰
        
        logger.info(f"Cleaned up data older than {cutoff_date}")
        return {'status': 'success', 'cleaned_before': cutoff_date.isoformat()}
        
    except Exception as e:
        logger.error(f"Error cleaning up old data: {str(e)}")
        return {'status': 'error', 'error': str(e)}


@shared_task
def process_analysis_task(task_id: str, parameters: dict):
    """å¤„ç†åˆ†æä»»åŠ¡ - ä½¿ç”¨WhenTradeGraphæ‰§è¡ŒçœŸå®åˆ†æ"""
    try:
        logger.info(f"Starting analysis task {task_id}")
        logger.info(f"Parameters: {parameters}")
        
        # å‘å¸ƒå¼€å§‹è¿›åº¦
        redis_publisher.publish_progress(task_id, 0, "åˆå§‹åŒ–åˆ†æç³»ç»Ÿ...")
        
        # è·å–åˆ†æå‚æ•° - ç›´æ¥ä»æ‰å¹³åŒ–ç»“æ„è·å–
        symbol = parameters.get('symbol', 'BTC/USDT')
        market_type = parameters.get('market_type', 'crypto')
        analysis_scopes = parameters.get('analysis_scopes', ['technical'])
        llm_provider = parameters.get('llm_provider', 'deepseek')
        llm_model = parameters.get('llm_model', 'deepseek-chat')
        
        # é…ç½®LLM
        config = WHENTRADE_CONFIG.copy()
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡çŠ¶æ€
        import os
        kimi_key = os.getenv("KIMI_API_KEY")
        deepseek_key = os.getenv("DEEPSEEK_API_KEY")
        logger.info(f"ğŸ” ç¯å¢ƒå˜é‡æ£€æŸ¥ - KIMI_API_KEY: {'å­˜åœ¨' if kimi_key else 'ä¸å­˜åœ¨'} | DEEPSEEK_API_KEY: {'å­˜åœ¨' if deepseek_key else 'ä¸å­˜åœ¨'}")
        if kimi_key:
            logger.info(f"ğŸŒ™ KIMI_API_KEY: {kimi_key[:20]}...")
        if deepseek_key:
            logger.info(f"ğŸ¤– DEEPSEEK_API_KEY: {deepseek_key[:20]}...")
        
        # æ£€æŸ¥å¹¶é…ç½®å¯ç”¨çš„LLMæä¾›å•†
        kimi_available = llm_config_service.is_provider_available("kimi")
        deepseek_available = llm_config_service.is_provider_available("deepseek")
        logger.info(f"ğŸ” LLMæä¾›å•†å¯ç”¨æ€§ - Kimi: {kimi_available} | DeepSeek: {deepseek_available}")
        
        if kimi_available:
            config["llm_provider"] = "openai"  # Kimiä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            config["deep_think_llm"] = "moonshot-v1-128k"
            config["quick_think_llm"] = "moonshot-v1-32k"
            config["backend_url"] = "https://api.moonshot.cn/v1"
            logger.info("Using Kimi as LLM provider")
        elif deepseek_available:
            config["llm_provider"] = "deepseek"
            config["deep_think_llm"] = "deepseek-chat"
            config["quick_think_llm"] = "deepseek-chat"
            config["backend_url"] = "https://api.deepseek.com/v1"
            logger.info("Using DeepSeek as LLM provider")
        else:
            # å¦‚æœæ²¡æœ‰å¯ç”¨çš„LLMï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ
            logger.warning("No LLM provider available, using mock analysis")
            return run_mock_analysis(task_id, symbol, analysis_scopes)
        
        redis_publisher.publish_progress(task_id, 10, "LLMé…ç½®å®Œæˆï¼Œå‡†å¤‡åˆ†æ...")
        
        # æ ¹æ®åˆ†æèŒƒå›´ç¡®å®šåˆ†æå¸ˆ
        selected_analysts = []
        if "technical" in analysis_scopes or "price" in analysis_scopes:
            selected_analysts.append("market")
        if "sentiment" in analysis_scopes or "social" in analysis_scopes:
            selected_analysts.extend(["social", "news"])
        if "onchain" in analysis_scopes:
            selected_analysts.append("fundamentals")
        if not selected_analysts:
            selected_analysts = ["market", "social", "news"]
        
        redis_publisher.publish_progress(task_id, 20, f"é€‰å®šåˆ†æå¸ˆ: {', '.join(selected_analysts)}")
        
        try:
            # åˆ›å»ºWhenTradeGraphå®ä¾‹
            graph = WhenTradeGraph(
                selected_analysts=selected_analysts,
                debug=False,
                config=config
            )
            
            redis_publisher.publish_progress(task_id, 30, "åˆ†æç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
            # å®šä¹‰åˆ†æé˜¶æ®µï¼ˆæ ¹æ®å¸‚åœºç±»å‹ï¼‰
            stages = []
            if market_type == 'polymarket':
                stages = [
                    ("analyst", "åˆ†æå›¢é˜Ÿ", ["market", "social", "news"]),
                    ("research", "ç ”ç©¶å›¢é˜Ÿ", ["yes", "no", "arbiter"]),
                    ("probability", "æ¦‚ç‡è¯„ä¼°", ["bayesian", "statistical"]),
                    ("strategy", "ç­–ç•¥åˆ¶å®š", ["position", "timing", "hedging"]),
                    ("decision", "å†³ç­–æ€»ç»“", ["decision"])
                ]
            else:
                stages = [
                    ("analyst", "åˆ†æå›¢é˜Ÿ", selected_analysts),
                    ("research", "ç ”ç©¶å›¢é˜Ÿ", ["bull", "bear", "manager"]),
                    ("trading", "äº¤æ˜“å›¢é˜Ÿ", ["trader"]),
                    ("risk", "é£é™©ç®¡ç†", ["risky", "neutral", "safe"]),
                    ("portfolio", "ç»„åˆç®¡ç†", ["portfolio"])
                ]
            
            # æ‰§è¡Œåˆ†æï¼ˆä½¿ç”¨propagateæ–¹æ³•ï¼‰
            logger.info(f"Starting WhenTradeGraph analysis for {symbol}")
            
            # å‘å¸ƒçœŸå®åˆ†æå¼€å§‹çŠ¶æ€
            redis_publisher.publish_progress(task_id, 30, f"å¼€å§‹åˆ†æ{symbol}...")
            
            # è°ƒç”¨WhenTradeGraphçš„propagateæ–¹æ³•
            target = symbol.split("/")[0] if "/" in symbol else symbol
            trade_date = datetime.utcnow().strftime("%Y-%m-%d")
            
            logger.info(f"Calling WhenTradeGraph.propagate with target={target}, date={trade_date}")
            
            # åœ¨çœŸå®åˆ†ææœŸé—´å‘å¸ƒè¿›åº¦æ›´æ–°
            redis_publisher.publish_progress(task_id, 40, "å¸‚åœºåˆ†æå¸ˆæ­£åœ¨åˆ†æ...")
            
            # æ³¨æ„ï¼špropagateæ–¹æ³•å¯èƒ½éœ€è¦å¼‚æ­¥æ”¯æŒï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
            try:
                # æ‰§è¡ŒçœŸå®åˆ†æ (Linus: language parameter flows through data structure)
                language = parameters.get('language', 'zh-CN')  # Get language from parameters
                logger.info(f"ğŸŒ Using language: {language}")
                final_state, signal = graph.propagate(target, trade_date, language)
                
                logger.info(f"âœ… WhenTradeGraph.propagate æˆåŠŸå®Œæˆ")
                logger.info(f"ğŸ“Š final_state keys: {list(final_state.keys()) if isinstance(final_state, dict) else 'not a dict'}")
                logger.info(f"ğŸ“ˆ signal: {signal}")
                
                # åˆ†æå®Œæˆï¼Œæ›´æ–°è¿›åº¦
                redis_publisher.publish_progress(task_id, 85, "åˆ†æå®Œæˆï¼Œç”ŸæˆæŠ¥å‘Š...")
                
            except Exception as propagate_error:
                logger.error(f"âŒ WhenTradeGraph.propagate å¤±è´¥: {propagate_error}")
                logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘æ¨¡æ‹Ÿåˆ†æ
            
            redis_publisher.publish_progress(task_id, 90, "ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            
            # æ„å»ºåˆ†æç»“æœ
            result = {
                "target": symbol,
                "timestamp": datetime.utcnow().isoformat(),
                "analysis_scopes": analysis_scopes,
                "summary": {
                    "overall_sentiment": final_state.get("final_trade_decision", "æŒæœ‰"),
                    "confidence": 0.75,
                    "recommendation": signal if signal else "æŒæœ‰è§‚æœ›"
                },
                "key_findings": [
                    {"type": "technical", "finding": "æŠ€æœ¯åˆ†æå®Œæˆ", "importance": "high"},
                    {"type": "sentiment", "finding": "å¸‚åœºæƒ…ç»ªåˆ†æå®Œæˆ", "importance": "medium"}
                ],
                "detailed_analysis": {
                    "market_report": final_state.get("market_report", ""),
                    "sentiment_report": final_state.get("sentiment_report", ""),
                    "news_report": final_state.get("news_report", ""),
                    "fundamentals_report": final_state.get("fundamentals_report", "")
                }
            }
            
        except Exception as e:
            logger.error(f"WhenTradeGraph analysis failed: {str(e)}\n{traceback.format_exc()}")
            # é™çº§åˆ°æ¨¡æ‹Ÿåˆ†æ
            return run_mock_analysis(task_id, symbol, analysis_scopes)
        
        # å‘å¸ƒå®Œæˆæ¶ˆæ¯
        redis_publisher.publish_complete(task_id, result)
        
        logger.info(f"Completed analysis task {task_id}")
        return {
            'task_id': task_id,
            'status': 'completed',
            'result': result,
            'timestamp': datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error processing analysis task {task_id}: {str(e)}\n{traceback.format_exc()}")
        redis_publisher.publish_error(task_id, str(e))
        return {
            'task_id': task_id,
            'status': 'failed',
            'error': str(e),
            'timestamp': datetime.utcnow().isoformat()
        }


def run_mock_analysis(task_id: str, symbol: str, analysis_scopes: list):
    """è¿è¡Œæ¨¡æ‹Ÿåˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
    logger.info(f"Running mock analysis for {task_id}")
    
    # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
    stages = ["åˆå§‹åŒ–", "æ•°æ®æ”¶é›†", "æŠ€æœ¯åˆ†æ", "æƒ…ç»ªåˆ†æ", "ç”ŸæˆæŠ¥å‘Š"]
    for i, stage in enumerate(stages):
        progress = int((i + 1) / len(stages) * 100)
        redis_publisher.publish_progress(task_id, progress, f"æ¨¡æ‹Ÿåˆ†æ: {stage}")
        
        # æ¨¡æ‹Ÿå»¶è¿Ÿ
        import time
        time.sleep(1)
    
    # æ¨¡æ‹Ÿç»“æœ
    result = {
        "target": symbol,
        "timestamp": datetime.utcnow().isoformat(),
        "analysis_scopes": analysis_scopes,
        "summary": {
            "overall_sentiment": "è°¨æ…ä¹è§‚ï¼ˆæ¨¡æ‹Ÿï¼‰",
            "confidence": 0.65,
            "recommendation": "æŒæœ‰è§‚æœ›ï¼ˆæ¨¡æ‹Ÿï¼‰"
        },
        "key_findings": [
            {"type": "mock", "finding": "è¿™æ˜¯æ¨¡æ‹Ÿåˆ†æç»“æœ", "importance": "info"}
        ],
        "detailed_analysis": {
            "note": "å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ŒçœŸå®LLMæœåŠ¡ä¸å¯ç”¨"
        }
    }
    
    redis_publisher.publish_complete(task_id, result)
    
    return {
        'task_id': task_id,
        'status': 'completed',
        'result': result,
        'timestamp': datetime.utcnow().isoformat()
    }


def get_agent_display_name(agent_id: str) -> str:
    """è·å–Agentæ˜¾ç¤ºåç§°"""
    names = {
        "market": "å¸‚åœºåˆ†æå¸ˆ",
        "social": "ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ",
        "news": "æ–°é—»åˆ†æå¸ˆ",
        "fundamentals": "åŸºæœ¬é¢åˆ†æå¸ˆ",
        "bull": "å¤šå¤´ç ”ç©¶å‘˜",
        "bear": "ç©ºå¤´ç ”ç©¶å‘˜",
        "manager": "ç ”ç©¶ç»ç†",
        "trader": "äº¤æ˜“å‘˜",
        "risky": "æ¿€è¿›åˆ†æå¸ˆ",
        "neutral": "ä¸­æ€§åˆ†æå¸ˆ",
        "safe": "ä¿å®ˆåˆ†æå¸ˆ",
        "portfolio": "ç»„åˆç»ç†"
    }
    return names.get(agent_id, agent_id)


@shared_task
def send_notification(user_id: str, message: str, notification_type: str = 'info'):
    """å‘é€é€šçŸ¥"""
    try:
        logger.info(f"Sending {notification_type} notification to user {user_id}: {message}")
        
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„é€šçŸ¥æœåŠ¡
        # ä¾‹å¦‚ï¼šé‚®ä»¶ã€æ¨é€é€šçŸ¥ã€WebSocketç­‰
        
        notification = {
            'user_id': user_id,
            'message': message,
            'type': notification_type,
            'timestamp': datetime.utcnow().isoformat(),
            'status': 'sent'
        }
        
        return notification
        
    except Exception as e:
        logger.error(f"Error sending notification to user {user_id}: {str(e)}")
        return {
            'user_id': user_id,
            'status': 'failed',
            'error': str(e)
        }