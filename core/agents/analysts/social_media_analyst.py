from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import time
import json
from datetime import datetime

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿå’Œåˆ†ææ¨¡å—æ—¥å¿—è£…é¥°å™¨
from core.utils.logging_init import get_logger
from core.utils.tool_logging import log_analyst_module
# å¯¼å…¥æç¤ºè¯åŠ è½½å™¨
from core.agents.prompt_loader import get_prompt_loader
# å¯¼å…¥Rediså‘å¸ƒå™¨ç”¨äºå‘é€å·¥å…·æ‰§è¡Œäº‹ä»¶
from core.services.redis_pubsub import redis_publisher
# å¯¼å…¥å¤šè¯­è¨€æ¶ˆæ¯ç³»ç»Ÿ
from core.i18n.messages import get_message, get_tool_name, get_agent_name

logger = get_logger("analysts.social_media")

# å·¥å…·åç§°ä¸­æ–‡æ˜ å°„
TOOL_NAME_CN = {
    'finnhub_news': 'Finnhubæ–°é—»',
    'reddit_sentiment': 'Redditæƒ…ç»ª',
    'sentiment_batch': 'æ‰¹é‡æƒ…ç»ªåˆ†æ',
    'get_reddit_stock_info': 'Redditè‚¡ç¥¨ä¿¡æ¯',
    'get_reddit_news': 'Redditæ–°é—»',
    'get_chinese_social_sentiment': 'ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ª',
    'get_finnhub_company_insider_sentiment': 'å…¬å¸å†…éƒ¨äººå‘˜æƒ…ç»ª',
    'get_stock_sentiment_unified': 'ç»Ÿä¸€æƒ…ç»ªåˆ†æ',
}


def _calculate_sentiment_time_params(timeframe: str, current_date: str) -> dict:
    """
    æ ¹æ®timeframeè®¡ç®—æƒ…ç»ªåˆ†æçš„æ—¶é—´å‚æ•°
    
    Args:
        timeframe: æ—¶é—´æ¡†æ¶ ('1å¤©', '1å‘¨', '1æœˆ', '1å¹´' æˆ– '1d', '1w', '1m', '1y')  
        current_date: å½“å‰æ—¥æœŸå­—ç¬¦ä¸²
        
    Returns:
        åŒ…å«days_backç­‰å‚æ•°çš„å­—å…¸
    """
    from datetime import datetime, timedelta
    
    # timeframeæ˜ å°„ï¼ˆæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼‰
    timeframe_mapping = {
        '1å¤©': 3,    # 1å¤©æ—¶é—´æ¡†æ¶ï¼ŒæŸ¥çœ‹3å¤©æƒ…ç»ªæ•°æ®
        '1å‘¨': 7,    # 1å‘¨æ—¶é—´æ¡†æ¶ï¼ŒæŸ¥çœ‹7å¤©æƒ…ç»ªæ•°æ®
        '1æœˆ': 14,   # 1æœˆæ—¶é—´æ¡†æ¶ï¼ŒæŸ¥çœ‹14å¤©æƒ…ç»ªæ•°æ®
        '1å¹´': 30,   # 1å¹´æ—¶é—´æ¡†æ¶ï¼ŒæŸ¥çœ‹30å¤©æƒ…ç»ªæ•°æ®
        '1d': 3,
        '1w': 7,
        '1m': 14,
        '1y': 30,
        '1h': 1,     # 1å°æ—¶å›¾æŸ¥çœ‹1å¤©æƒ…ç»ª
        '4h': 3,     # 4å°æ—¶å›¾æŸ¥çœ‹3å¤©æƒ…ç»ª
    }
    
    # è·å–å¤©æ•°ï¼Œé»˜è®¤7å¤©
    days_back = timeframe_mapping.get(timeframe, 7)
    
    return {
        'days_back': days_back,
        'max_results': min(20, days_back * 2)  # ç»“æœæ•°é‡ä¸æ—¶é—´èŒƒå›´æˆæ­£æ¯”
    }


def _construct_sentiment_tool_args(tool_id: str, symbol: str, time_params: dict) -> dict:
    """
    æ ¹æ®å·¥å…·IDå’Œæ—¶é—´å‚æ•°æ„é€ æƒ…ç»ªå·¥å…·è°ƒç”¨å‚æ•°
    
    Args:
        tool_id: å·¥å…·ID (å¦‚ 'finnhub_news', 'reddit_sentiment')
        symbol: äº¤æ˜“æ ‡çš„
        time_params: æ—¶é—´å‚æ•°å­—å…¸
        
    Returns:
        å·¥å…·è°ƒç”¨å‚æ•°å­—å…¸
    """
    # ğŸ”§ ä¿®å¤ï¼šæ— å‚æ•°å·¥å…· - Linuså¼ç»Ÿä¸€å¤„ç†
    no_param_tools = ['fear_greed', 'market_overview', 'global_market_cap']
    if tool_id in no_param_tools:
        logger.debug(f"ğŸ”§ [æ— å‚æ•°å·¥å…·] {tool_id} ä¸éœ€è¦å‚æ•°")
        return {}
    
    # æƒ…ç»ªåˆ†æå·¥å…·å‚æ•°æ˜ å°„ (ä½¿ç”¨æ­£ç¡®çš„å·¥å…·ID)
    sentiment_tools = {
        'finnhub_news': {
            'symbol': symbol,
            'days_back': time_params['days_back'],
            'max_results': time_params['max_results']
        },
        'reddit_sentiment': {
            'symbol': symbol,
            'days_back': time_params['days_back'],
            'max_results': time_params['max_results']
        },
        'sentiment_batch': {
            'symbol': symbol,
            'sources': ['finnhub', 'reddit'],
            'days_back': time_params['days_back']
        }
    }
    
    # è¿”å›å¯¹åº”å·¥å…·çš„å‚æ•°ï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å›åŸºæœ¬å‚æ•°
    return sentiment_tools.get(tool_id, {'symbol': symbol})


def create_social_media_analyst(llm, toolkit):
    @log_analyst_module("social_media")
    def social_media_analyst_node(state):
        current_date = state["trade_date"]
        ticker = state["company_of_interest"]
        company_name = state["company_of_interest"]

        # ğŸ›  ä½¿ç”¨Toolkitä¸­å·²æœ‰çš„å·¥å…·ï¼Œé¿å…å·¥å…·ä¸åŒ¹é…é”™è¯¯
        logger.info(f"[ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] ä½¿ç”¨Toolkitä¸­çš„æ ‡å‡†å·¥å…·è·å–{ticker}çš„å¸‚åœºæƒ…ç»ª")
        
        # LinusåŸåˆ™ï¼šç»Ÿä¸€ä½¿ç”¨ç›´æ¥æ‰§è¡Œæ¨¡å¼ï¼Œæ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ
        logger.info(f"ğŸ›  [ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] ä½¿ç”¨ç»Ÿä¸€çš„ç›´æ¥æ‰§è¡Œæ¨¡å¼")
        
        # è·å–analysis_idç”¨äºå‘é€WebSocketäº‹ä»¶
        analysis_id = state.get("analysis_id")
        
        # å¦‚æœæ²¡æœ‰analysis_idï¼Œè®°å½•è­¦å‘Š
        if not analysis_id:
            logger.warning(f"âš ï¸ [ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] æ²¡æœ‰analysis_idï¼Œå·¥å…·æ‰§è¡Œæ¶ˆæ¯å°†æ— æ³•å‘é€")
        else:
            logger.info(f"âœ… [ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] ä½¿ç”¨analysis_id: {analysis_id}")
        
        # è·å–ç”¨æˆ·é€‰æ‹©çš„æ—¶é—´æ¡†æ¶å¹¶è®¡ç®—æ—¶é—´å‚æ•°
        timeframe = state.get("timeframe", "1d")
        time_params = _calculate_sentiment_time_params(timeframe, current_date)
        logger.info(f"ğŸ“… [ç›´æ¥æ‰§è¡Œ] åŸºäºtimeframe '{timeframe}' è®¡ç®—æƒ…ç»ªæ—¶é—´å‚æ•°: {time_params}")
        
        # ğŸ›  ä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼šæ­£ç¡®çš„å·¥å…·åˆ†ç±»é€»è¾‘
        selected_tools = state.get("selected_tools", [])
        logger.info(f"ğŸ” [å·¥å…·åˆ†ç±»] ç”¨æˆ·åŸå§‹é€‰æ‹©: {selected_tools}")
        
        # ğŸ›  Linuså¼è§£å†³æ–¹æ¡ˆï¼šé‡æ–°è®¾è®¡æ•°æ®ç»“æ„
        # å®šä¹‰æƒ…ç»ªåˆ†æå¸ˆçœŸæ­£éœ€è¦çš„å·¥å…·
        SENTIMENT_TOOL_MAPPING = {
            # ç”¨æˆ·å¯èƒ½é€‰æ‹©çš„å·¥å…·ID -> å®é™…çš„æƒ…ç»ªå·¥å…·æ–¹æ³•
            'finnhub_news': 'get_finnhub_crypto_news',
            'reddit_sentiment': 'get_crypto_reddit_sentiment', 
            'sentiment_batch': 'analyze_sentiment_batch',
            'fear_greed': 'get_fear_greed_index',
            # æŠ€æœ¯å·¥å…·æ— æ³•ç”¨äºæƒ…ç»ªåˆ†æï¼Œä½†ä¸æŠ¥é”™ï¼Œç›´æ¥å¿½ç•¥
            'crypto_price': None,
            'indicators': None,
            'market_data': None,
            'historical_data': None,
        }
        
        # ä»ç”¨æˆ·é€‰æ‹©ä¸­ç­›é€‰å‡ºçœŸæ­£å¯ç”¨çš„æƒ…ç»ªå·¥å…·
        available_sentiment_tools = []
        for tool_id in selected_tools:
            mapped_method = SENTIMENT_TOOL_MAPPING.get(tool_id)
            if mapped_method:  # ä¸æ˜¯None
                # æ£€æŸ¥toolkitä¸­æ˜¯å¦æœ‰å¯¹åº”çš„å·¥å…·IDï¼ˆè€Œéæ–¹æ³•åï¼‰- Linuså¼ç®€åŒ–
                if hasattr(toolkit, tool_id):
                    available_sentiment_tools.append(tool_id)
                    logger.info(f"âœ… [å·¥å…·æ˜ å°„] {tool_id} (å­˜åœ¨)")
                else:
                    logger.warning(f"âš ï¸ [å·¥å…·æ˜ å°„] {tool_id} (ä¸å­˜åœ¨)")
            elif mapped_method is None and tool_id in SENTIMENT_TOOL_MAPPING:
                # æŠ€æœ¯å·¥å…·ï¼Œç›´æ¥å¿½ç•¥ï¼Œä¸æŠ¥é”™
                logger.debug(f"ğŸ›  [å·¥å…·è¿‡æ»¤] {tool_id} æ˜¯æŠ€æœ¯å·¥å…·ï¼Œè·³è¿‡")
            else:
                # æœªçŸ¥å·¥å…·
                logger.warning(f"â“ [å·¥å…·æ˜ å°„] æœªçŸ¥å·¥å…·: {tool_id}")
        
        logger.info(f"ğŸ¯ [å·¥å…·åˆ†ç±»ç»“æœ] å¯ç”¨æƒ…ç»ªå·¥å…·: {available_sentiment_tools}")
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„æƒ…ç»ªå·¥å…·ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if not available_sentiment_tools:
            logger.info(f"ğŸ”„ [é»˜è®¤é…ç½®] ç”¨æˆ·æœªé€‰æ‹©æƒ…ç»ªå·¥å…·ï¼Œä½¿ç”¨é»˜è®¤çš„Redditæƒ…ç»ªåˆ†æ")
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸºç¡€çš„æƒ…ç»ªåˆ†æå·¥å…·å¯ç”¨ - Linuså¼ç®€åŒ–
            default_tools = []
            if hasattr(toolkit, 'reddit_sentiment'):
                default_tools.append('reddit_sentiment')
            if hasattr(toolkit, 'fear_greed'):
                default_tools.append('fear_greed')
            
            available_sentiment_tools = default_tools
            logger.info(f"ğŸ”„ [é»˜è®¤é…ç½®] ä½¿ç”¨é»˜è®¤å·¥å…·: {available_sentiment_tools}")
        
        # è·å–è¯­è¨€è®¾ç½®ï¼ˆä¿®å¤ä½œç”¨åŸŸé—®é¢˜ï¼‰
        language = state.get("language", "zh-CN")
        
        # å‘é€å·¥å…·æ‰§è¡Œå¼€å§‹èšåˆæ¶ˆæ¯
        start_time = datetime.utcnow()
        if analysis_id and redis_publisher and available_sentiment_tools:
            try:
                # ä½¿ç”¨æœ¬åœ°åŒ–å·¥å…·åç§°ï¼ˆä¸å¸‚åœºåˆ†æå¸ˆä¿æŒä¸€è‡´ï¼‰
                tools_localized_list = [get_tool_name(tool_id, language) for tool_id in available_sentiment_tools]
                tools_list = ", ".join(tools_localized_list)
                
                # æ‰‹åŠ¨æ„å»ºæ¶ˆæ¯ï¼ˆä¸å¸‚åœºåˆ†æå¸ˆä¸€è‡´ï¼‰
                start_msg = get_message('tool_execution_start', language)
                tools_count_label = get_message('tools_count', language)
                total_count_label = get_message('total_count', language)
                colon = get_message('colon', language)
                agent_name = get_agent_name('social_media', language)
                message = f"{start_msg}{colon} {tools_list} ({total_count_label} {len(available_sentiment_tools)} {tools_count_label})"
                
                redis_publisher.client.publish(
                    f"analysis:{analysis_id}",
                    json.dumps({
                        "type": "agent.tool",
                        "data": {
                            "analysisId": analysis_id,
                            "agent": agent_name,
                            "tool": "batch_execution",
                            "status": "executing",
                            "message": message,
                            "timestamp": start_time.isoformat()
                        }
                    })
                )
                logger.debug(f"ğŸ“¡ [èšåˆæ¶ˆæ¯] å·²å‘é€æƒ…ç»ªå·¥å…·æ‰¹é‡æ‰§è¡Œå¼€å§‹äº‹ä»¶: {len(available_sentiment_tools)}ä¸ªå·¥å…·")
            except Exception as e:
                logger.warning(f"âš ï¸ [èšåˆæ¶ˆæ¯] å‘é€å·¥å…·å¼€å§‹äº‹ä»¶å¤±è´¥: {e}")

        # ç›´æ¥æ‰§è¡Œæ‰€æœ‰å¯ç”¨çš„æƒ…ç»ªå·¥å…·
        tool_results = []
        successful_tools = 0
        failed_tools = 0
        
        for tool_id in available_sentiment_tools:
            try:
                tool_cn_name = TOOL_NAME_CN.get(tool_id, tool_id)
                logger.info(f"ğŸ¯ [ç›´æ¥æ‰§è¡Œ] æ­£åœ¨æ‰§è¡Œæƒ…ç»ªå·¥å…·: {tool_cn_name} ({tool_id})")
                
                # ğŸ›  ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨tool_idæŸ¥æ‰¾å·¥å…· - Linuså¼ç®€åŒ–
                # Toolkitæ³¨å†Œçš„æ˜¯tool_idï¼Œä¸æ˜¯method_name
                tool_method = getattr(toolkit, tool_id, None)
                method_name = SENTIMENT_TOOL_MAPPING.get(tool_id)  # ä»…ç”¨äºæ—¥å¿—æ˜¾ç¤º
                
                if tool_method is None:
                    # ğŸ›  å¢å¼ºï¼šæ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                    available_methods = [attr for attr in dir(toolkit) if not attr.startswith('_') and callable(getattr(toolkit, attr, None))]
                    toolkit_selected_tools = getattr(toolkit, 'selected_tools', [])
                    
                    logger.error(f"âŒ [ç›´æ¥æ‰§è¡Œ] æƒ…ç»ªå·¥å…·æ–¹æ³•æœªæ‰¾åˆ°: {method_name or tool_id}")
                    logger.error(f"   ğŸ“Š Toolkitæ³¨å†Œçš„å·¥å…·: {toolkit_selected_tools}")
                    logger.error(f"   ğŸ” Toolkitå¯ç”¨æ–¹æ³•(å‰20ä¸ª): {available_methods[:20]}")
                    logger.error(f"   ğŸ›  é¢„æœŸæ–¹æ³•å: {method_name}")
                    
                    tool_results.append({
                        "tool": tool_id,
                        "result": {
                            "error": f"æƒ…ç»ªå·¥å…·æ–¹æ³•æœªæ‰¾åˆ°: {method_name or tool_id}",
                            "symbol": ticker,
                            "tool_id": tool_id,
                            "method_name": method_name,
                            "available_methods": available_methods[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªé¿å…æ—¥å¿—è¿‡é•¿
                        }
                    })
                    failed_tools += 1
                    continue
                
                # æ ¹æ®å·¥å…·IDæ„é€ å‚æ•°
                tool_args = _construct_sentiment_tool_args(tool_id, ticker, time_params)
                logger.debug(f"ğŸ›  [ç›´æ¥æ‰§è¡Œ] å·¥å…·å‚æ•°: {tool_args}")
                
                # æ‰§è¡Œå·¥å…·
                result_data = tool_method(**tool_args)
                tool_results.append({
                    "tool": tool_id,
                    "result": result_data
                })
                logger.info(f"âœ… [ç›´æ¥æ‰§è¡Œ] å·¥å…·{tool_cn_name}æ‰§è¡ŒæˆåŠŸ")
                successful_tools += 1
                    
            except Exception as e:
                logger.error(f"âŒ [ç›´æ¥æ‰§è¡Œ] å·¥å…·{tool_id}æ‰§è¡Œå¤±è´¥: {str(e)}")
                tool_results.append({
                    "tool": tool_id,
                    "result": {
                        "error": str(e),
                        "symbol": ticker,
                        "tool_id": tool_id
                    }
                })
                failed_tools += 1

        # å‘é€å·¥å…·æ‰§è¡Œå®Œæˆèšåˆæ¶ˆæ¯
        if analysis_id and redis_publisher and available_sentiment_tools:
            try:
                end_time = datetime.utcnow()
                duration = (end_time - start_time).total_seconds()
                
                # æ‰‹åŠ¨æ„å»ºå®Œæˆæ¶ˆæ¯ï¼ˆä¸å¸‚åœºåˆ†æå¸ˆä¸€è‡´ï¼‰
                complete_msg = get_message('tool_execution_complete', language)
                tools_label = get_message('tools_count', language)
                success_label = get_message('success_count', language)
                failed_label = get_message('failed_count', language)
                time_label = get_message('time_spent', language)
                agent_name = get_agent_name('social_media', language)
                
                comma = get_message('comma', language)
                total_count_label = get_message('total_count', language)
                message = f"{complete_msg}{comma} {total_count_label} {len(available_sentiment_tools)} {tools_label}{comma} {successful_tools} {success_label}{comma} {failed_tools} {failed_label}{comma} {time_label} {duration:.1f}s"
                
                redis_publisher.client.publish(
                    f"analysis:{analysis_id}",
                    json.dumps({
                        "type": "agent.tool",
                        "data": {
                            "analysisId": analysis_id,
                            "agent": agent_name,
                            "tool": "batch_execution",
                            "status": "completed",
                            "message": message,
                            "timestamp": end_time.isoformat()
                        }
                    })
                )
                logger.debug(f"ğŸ“¡ [èšåˆæ¶ˆæ¯] å·²å‘é€æƒ…ç»ªå·¥å…·æ‰¹é‡æ‰§è¡Œå®Œæˆäº‹ä»¶: è€—æ—¶{duration:.1f}s")
            except Exception as e:
                logger.warning(f"âš ï¸ [èšåˆæ¶ˆæ¯] å‘é€å·¥å…·å®Œæˆäº‹ä»¶å¤±è´¥: {e}")
        
        # è·å–è¯­è¨€å‚æ•°ï¼ˆä»stateä¸­æå–ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤ä¸­æ–‡ï¼‰
        language = state.get("language", "zh-CN")
        
        # ä½¿ç”¨æç¤ºè¯åŠ è½½å™¨è·å–é…ç½®ï¼ˆæ”¯æŒå¤šè¯­è¨€ï¼‰
        prompt_loader = get_prompt_loader()
        prompt_config = prompt_loader.load_prompt("social_media_analyst", language=language)
        
        # è·å–ç³»ç»Ÿæ¶ˆæ¯
        system_message = prompt_config.get("system_message", "æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“åˆ†æå¸ˆã€‚")
        
        # è®°å½•æç¤ºè¯ç‰ˆæœ¬
        prompt_version = prompt_loader.get_prompt_version("social_media_analyst", language=language)
        logger.debug(f"ğŸ­ [DEBUG] ä½¿ç”¨æç¤ºè¯ç‰ˆæœ¬: {prompt_version} (è¯­è¨€: {language})")
        
        # ğŸ›  ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å·¥å…·ç»“æœï¼Œé¿å…"æ•°æ®ä¸­æ²¡æœ‰æä¾›"çš„è¯¯å¯¼
        if tool_results:
            # è¿‡æ»¤æ‰å¤±è´¥çš„å·¥å…·ç»“æœï¼Œåªä¿ç•™æˆåŠŸçš„
            successful_results = [r for r in tool_results if 'error' not in str(r['result'])]
            failed_results = [r for r in tool_results if 'error' in str(r['result'])]
            
            if successful_results:
                tool_results_text = "\n\n".join([
                    f"## {r['tool']}\n{r['result']}" for r in successful_results
                ])
                system_content = (
                    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æå¸ˆã€‚åŸºäºä»¥ä¸‹å·¥å…·è·å–çš„çœŸå®æ•°æ®è¿›è¡Œåˆ†æã€‚\n\n"
                    "å·¥å…·æ•°æ®ï¼š\n{tool_results}\n\n"
                    "{system_message}\n\n"
                    "é‡è¦ï¼šè¯·ç›´æ¥åŸºäºä¸Šè¿°çœŸå®æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦è¯´'æ•°æ®ä¸­æ²¡æœ‰æä¾›'ä¹‹ç±»çš„è¯ã€‚"
                    "å¦‚æœæŸä¸ªå¹³å°çš„æ•°æ®ä¸å¯ç”¨ï¼Œè¯·ä¸“æ³¨äºåˆ†æå¯ç”¨çš„æ•°æ®ã€‚"
                )
                if failed_results:
                    logger.info(f"ğŸ“Š æˆåŠŸå·¥å…·: {len(successful_results)}, å¤±è´¥å·¥å…·: {len(failed_results)}")
            else:
                # æ‰€æœ‰å·¥å…·éƒ½å¤±è´¥äº†
                tool_results_text = "æ‰€æœ‰æƒ…ç»ªå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œæ— æ³•è·å–å®æ—¶æ•°æ®ã€‚"
                system_content = (
                    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æå¸ˆã€‚\n\n"
                    "{system_message}\n\n"
                    "æ³¨æ„ï¼šæ— æ³•è·å–å®æ—¶ç¤¾äº¤åª’ä½“æ•°æ®ï¼Œè¯·åŸºäºä¸€èˆ¬å¸‚åœºçŸ¥è¯†è¿›è¡Œæƒ…ç»ªåˆ†æã€‚\n"
                    "è¯·åˆ†æå½“å‰å¸‚åœºå¯¹è¯¥åŠ å¯†è´§å¸çš„ä¸€èˆ¬æƒ…ç»ªè¶‹åŠ¿ï¼Œå¹¶æ˜ç¡®è¯´æ˜è¿™æ˜¯åŸºäºä¸€èˆ¬å¸‚åœºçŸ¥è¯†çš„åˆ†æã€‚"
                )
        else:
            tool_results_text = "æœªæ‰§è¡Œæƒ…ç»ªå·¥å…·è°ƒç”¨ã€‚"
            system_content = (
                "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æå¸ˆã€‚\n\n"
                "{system_message}\n\n"
                "æ³¨æ„ï¼šæœªæ‰§è¡Œæƒ…ç»ªå·¥å…·è°ƒç”¨ï¼Œè¯·åŸºäºä¸€èˆ¬å¸‚åœºçŸ¥è¯†è¿›è¡Œæƒ…ç»ªåˆ†æã€‚\n"
                "è¯·åˆ†æå½“å‰å¸‚åœºå¯¹è¯¥åŠ å¯†è´§å¸çš„ä¸€èˆ¬æƒ…ç»ªè¶‹åŠ¿ï¼Œå¹¶æ˜ç¡®è¯´æ˜è¿™æ˜¯åŸºäºä¸€èˆ¬å¸‚åœºçŸ¥è¯†çš„åˆ†æã€‚"
            )
            
        # ä½¿ç”¨LLMåˆ†æå·¥å…·ç»“æœ
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", system_content),
            MessagesPlaceholder(variable_name="messages")
        ])
        
        analysis_prompt = analysis_prompt.partial(tool_results=tool_results_text)
        analysis_prompt = analysis_prompt.partial(system_message=system_message)
        
        # ğŸ›  Tokenæ§åˆ¶ï¼šæ™ºèƒ½å‹ç¼©æ¶ˆæ¯é¿å…32768é™åˆ¶
        from core.utils.token_manager import compress_messages_smart
        
        original_messages = state.get("messages", [])
        logger.debug(f"ğŸ” [TokenManager] åŸå§‹æ¶ˆæ¯æ•°é‡: {len(original_messages)}")
        
        # æ™ºèƒ½å‹ç¼©æ¶ˆæ¯
        compressed_messages, token_usage = compress_messages_smart(original_messages, max_tokens=32768)
        
        if len(compressed_messages) < len(original_messages):
            logger.info(f"ğŸ“¦ [TokenManager] æ¶ˆæ¯å‹ç¼©: {len(original_messages)} -> {len(compressed_messages)}, tokens: ~{token_usage.estimated_tokens}")
        else:
            logger.debug(f"âœ… [TokenManager] æ¶ˆæ¯æ— éœ€å‹ç¼©, tokens: ~{token_usage.estimated_tokens}")
        
        # ğŸ›  Linuså¼ä¿®å¤ï¼šç»Ÿä¸€é”™è¯¯å¤„ç†ï¼Œæ¶ˆé™¤å´©æºƒç‰¹æ®Šæƒ…å†µ
        try:
            analysis_chain = analysis_prompt | llm
            # ğŸ”´ è¯­è¨€å¼ºåˆ¶å‰ç¼€ - ç¡®ä¿LLMä¸¥æ ¼éµå¾ªé€‰å®šè¯­è¨€

            language = state.get("language", "zh-CN")

            language_name = "English" if language == "en-US" else "ç®€ä½“ä¸­æ–‡"

            language_prefix = f"[ğŸ”´ CRITICAL: Respond ONLY in {language_name}. No mixed languages. This overrides ALL other instructions.] "

            

            # åœ¨è°ƒç”¨LLMå‰æ·»åŠ è¯­è¨€å‰ç¼€åˆ°messages

            try:

                messages = state["messages"]

                if messages:

                    # åˆ›å»ºå¸¦å‰ç¼€çš„æ¶ˆæ¯å‰¯æœ¬

                    prefixed_messages = messages.copy()

                    # åœ¨ç¬¬ä¸€ä¸ªæ¶ˆæ¯å‰æ·»åŠ ç³»ç»Ÿçº§è¯­è¨€å‰ç¼€

                    from langchain_core.messages import SystemMessage

                    language_system_msg = SystemMessage(content=language_prefix)

                    prefixed_messages = [language_system_msg] + prefixed_messages

                    result = analysis_chain.invoke(prefixed_messages)

                else:

                    # ğŸ”´ è¯­è¨€å¼ºåˆ¶å‰ç¼€ - ç¡®ä¿LLMä¸¥æ ¼éµå¾ªé€‰å®šè¯­è¨€


                    language = state.get("language", "zh-CN")


                    language_name = "English" if language == "en-US" else "ç®€ä½“ä¸­æ–‡"


                    language_prefix = f"[ğŸ”´ CRITICAL: Respond ONLY in {language_name}. No mixed languages. This overrides ALL other instructions.] "


                    


                    # åœ¨è°ƒç”¨LLMå‰æ·»åŠ è¯­è¨€å‰ç¼€åˆ°compressed_messages


                    try:


                        if compressed_messages:


                            # åˆ›å»ºå¸¦å‰ç¼€çš„æ¶ˆæ¯å‰¯æœ¬


                            prefixed_compressed_messages = compressed_messages.copy()


                            # åœ¨ç¬¬ä¸€ä¸ªæ¶ˆæ¯å‰æ·»åŠ ç³»ç»Ÿçº§è¯­è¨€å‰ç¼€


                            from langchain_core.messages import SystemMessage


                            language_system_msg = SystemMessage(content=language_prefix)


                            prefixed_compressed_messages = [language_system_msg] + prefixed_compressed_messages


                            result = analysis_chain.invoke(prefixed_compressed_messages)


                        else:


                            result = analysis_chain.invoke(compressed_messages)


                    except Exception as e:


                        # é™çº§å¤„ç†ï¼šç›´æ¥è°ƒç”¨åŸæ–¹æ³•


                        logger.warning(f"âš ï¸ è¯­è¨€å‰ç¼€æ·»åŠ å¤±è´¥ï¼Œä½¿ç”¨åŸæ–¹æ³•: {e}")


                        result = analysis_chain.invoke(compressed_messages)

            except Exception as e:

                # é™çº§å¤„ç†ï¼šç›´æ¥è°ƒç”¨åŸæ–¹æ³•

                logger.warning(f"âš ï¸ è¯­è¨€å‰ç¼€æ·»åŠ å¤±è´¥ï¼Œä½¿ç”¨åŸæ–¹æ³•: {e}")

                # ğŸ”´ è¯­è¨€å¼ºåˆ¶å‰ç¼€ - ç¡®ä¿LLMä¸¥æ ¼éµå¾ªé€‰å®šè¯­è¨€


                language = state.get("language", "zh-CN")


                language_name = "English" if language == "en-US" else "ç®€ä½“ä¸­æ–‡"


                language_prefix = f"[ğŸ”´ CRITICAL: Respond ONLY in {language_name}. No mixed languages. This overrides ALL other instructions.] "


                


                # åœ¨è°ƒç”¨LLMå‰æ·»åŠ è¯­è¨€å‰ç¼€åˆ°compressed_messages


                try:


                    if compressed_messages:


                        # åˆ›å»ºå¸¦å‰ç¼€çš„æ¶ˆæ¯å‰¯æœ¬


                        prefixed_compressed_messages = compressed_messages.copy()


                        # åœ¨ç¬¬ä¸€ä¸ªæ¶ˆæ¯å‰æ·»åŠ ç³»ç»Ÿçº§è¯­è¨€å‰ç¼€


                        from langchain_core.messages import SystemMessage


                        language_system_msg = SystemMessage(content=language_prefix)


                        prefixed_compressed_messages = [language_system_msg] + prefixed_compressed_messages


                        result = analysis_chain.invoke(prefixed_compressed_messages)


                    else:


                        result = analysis_chain.invoke(compressed_messages)


                except Exception as e:


                    # é™çº§å¤„ç†ï¼šç›´æ¥è°ƒç”¨åŸæ–¹æ³•


                    logger.warning(f"âš ï¸ è¯­è¨€å‰ç¼€æ·»åŠ å¤±è´¥ï¼Œä½¿ç”¨åŸæ–¹æ³•: {e}")


                    result = analysis_chain.invoke(compressed_messages)
        except Exception as e:
            logger.error(f"âŒ [ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            # åˆ›å»ºé™çº§å“åº”ï¼Œç¡®ä¿æµç¨‹ç»§ç»­
            from langchain_core.messages import AIMessage
            result = AIMessage(content=f"ç¤¾äº¤åª’ä½“åˆ†ææš‚æ—¶ä¸å¯ç”¨ï¼š{str(e)}ã€‚è¯·æ£€æŸ¥LLMé…ç½®æˆ–tokené™åˆ¶ã€‚")
        
        # è¿”å›åˆ†æç»“æœ
        report = result.content if hasattr(result, 'content') else str(result)
        logger.info(f"ğŸ­ [ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] ç›´æ¥æ‰§è¡Œæ¨¡å¼åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šé•¿åº¦: {len(report)}")
        
        # é‡Šæ”¾åºåˆ—é”ï¼Œå…è®¸ä¸‹ä¸€ä¸ªåˆ†æå¸ˆå¼€å§‹æ‰§è¡Œ
        logger.info(f"ğŸ”“ [ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ] é‡Šæ”¾åºåˆ—é”ï¼Œå®Œæˆæ‰§è¡Œ")
        
        # æ·»åŠ å°å»¶è¿Ÿç¡®ä¿æ¶ˆæ¯å·²å‘é€å®Œæˆ
        import time
        time.sleep(0.5)
        
        return {
            "messages": [result],
            "sentiment_report": report,
            "current_sequence": None,  # é‡Šæ”¾å½“å‰åºåˆ—
            "sequence_lock": False,    # é‡Šæ”¾é”
        }


    return social_media_analyst_node
